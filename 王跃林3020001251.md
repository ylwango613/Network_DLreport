#  “博采众长”——基于原图RGB增强、卷积特征提取和多模型集成学习三种思路研究

## 摘要

### 研究目的



### 方法

### 实验结果

### 结论



## 引言

### 课题意义

​	在计算机视觉领域，卷积神经网络（CNN）与 Vision Transformer（ViT）的融合代表了一种创新的模型发展方向。CNN以其在图像处理中的局部感知能力和平移不变性而著称，这使其能够高效地捕捉图像中的细节和局部特征。相比之下，ViT通过其先进的自注意力机制，能够理解和利用图像的全局上下文信息。将这两种技术结合起来，我们可以构建出一个更为强大的模型，它不仅综合了局部特征的细致识别能力和全局信息的广泛捕捉能力，还能在各种数据规模上表现出色。具体来说，虽然ViT在处理大规模数据集时展现出卓越的性能，但其在小型数据集上的效果往往不如CNN。这种结合方式能显著提升模型在各种数据集上的泛化能力，从而使模型在不同的应用环境下都能保持一致的高性能。

​	此外，这种结合还有助于我们更深入地理解ViT和CNN各自的优势和局限性，以及它们如何相互补充。这不仅提高了模型的鲁棒性，也增强了模型的可解释性，这对于需要高度安全性和可靠性的应用场景尤为重要。

### 最新研究进展

​	在计算机视觉领域，卷积神经网络（CNN）与Vision Transformer（ViT）的融合一直是研究的热点之一。最新的研究表明，将CNN和ViT结合起来可以充分发挥它们各自的优势，取得更好的性能。一些研究通过设计新的模型架构，将CNN用于提取局部特征，然后将这些局部特征传递给ViT模块，以捕捉全局信息。这种融合方式在不同数据规模上都表现出色，从小型数据集到大规模数据集都能保持较好的性能[1]。还有一些研究试图找到一种方式能够让ViT工作在小型数据集当中，使用了一些图像增强相关的技术[2]。或者是借助数据蒸馏手段，先在一些得到广泛认可的大数据集上进行训练，再通过微调的方法适用于现实中广泛存在的小数据集[3]。同时还有一些研究者看重ViT在现实当中使用的相关问题，提出了基于移动设备ViT模型mobileViT，以mobileViT为背景做了一些优化和改进[4]。

### 当前存在的问题 

​	目前的研究主要聚焦于通过不同的途径提升Vision Transformer（ViT）在小数据集上性能的问题。这包括对原始图像进行增强和优化网络架构的调参，然而，迄今为止，相关工作在将这两方面结合起来方面仍显不足。此外，对于解决这类深度学习问题，人们主要关注网络参数和架构的优化，而较少考虑运用机器学习算法进行优化。

### 课题挑战的贡献

​	相较之下，我们的研究专注于采用有效的训练策略和技术，全面提高ViT在小数据集上的数据利用效率。强调从多角度协助ViT克服性能问题，填补了综合运用图像增强和网络架构优化的空白，突出了机器学习算法在优化深度学习问题中的潜在作用。总体而言，我们的研究旨在解决当前研究中整合图像增强和网络架构优化方面的挑战，推动更全面的深度学习问题优化方法。此外，我们的课题通过引入新的图像增强技术，结合CNN卷积特征提取，以及集成学习方法，最后的模型通过多分类器投票评选提高模型准确率，为当前领域的发展贡献了创新和综合性的解决方案。

### 主要技巧

​	本课题的核心技术涵盖了原始图像增强、基于不同卷积特征提取的ViT训练辅助，以及采用多模型组合进行集成学习的方法。在原始图像增强技术方面，我们采用了一种简单的方法，将原始的输入图像传递到每一个层中。同时，每一次卷积的结果不仅输入到下一个核，还输入到与其非直接相连的核，从而增强了层与层之间的联系。其次，对ViT进行拆解，包括embedding层、transform层和fc层，在生成patch的embedding层引入一些经典的卷积网络，通过卷积提取原始图像的特征，更好帮助ViT掌握信息。还可以在此基础上融合基于输入图像的图像增强技术，以统一局部信息和全局信息的特征提取，使得模型更全面地理解图像内容进而实现更准确的图像分类。

​	最后，我们基于不同的模型，以它们不同的学习策略为基础建立了集成学习算法，进一步提高了最终判定的准确性。这一算法整合了多个模型的预测结果，通过多分类器投票评选的方式，为最终的决策提供更为可靠的依据。这项研究不仅引入了新的图像增强技术，还结合了CNN卷积特征提取和集成学习方法，为领域的发展贡献了创新和综合性的解决方案。总体而言，我们致力于解决当前研究中整合图像增强和网络架构优化方面的挑战，推动更全面的深度学习问题优化方法。

在本节的最后一段中，我将对本技术报告的主要贡献总结如下：

1) 本报告提出了一种类ResNet的原始数据增强的方法，加强了神经网络层与层之间的联系；
2) 将多种CNN架构作为特征提取器和ViT结合起来，得到了ViT和CNN相结合的模型；
3) 考虑了基于集成学习思想的多模型共同参与投票决策的新类inception模型架构。

## 研究方法

### 研究思路

#### 原图RGB增强

​	原有的前馈神经网络强烈依靠前层传来的特征，只能看到前层神经网络提取后的特征，没有发挥出每一个卷积层的潜在优势。仿照ResNet的思想，使用类似残差连接的想法，在把每一层卷积结果不仅提供给下一层的基础上，还将原始图像输入到每一卷积层当中。

![image-20240112191523278](D:\Typora\Pic\image-20240112191523278.png)

#### 卷积特征提取

​	在原有ViT基础上主要考虑第一步embedding方法的改进，原有的方法只是简单的一层卷积，这里选择更为复杂的传统卷积模型LeNet5和ResNet18作为patch的特征提取器。还可以考虑再加上第一种的原图RGB增强方法，将原始图像和经过CNN提取的特征一起作为ViT模型的输入，加强对待图像分类物体的特征提取。

![image-20240112192118312](D:\Typora\Pic\image-20240112192118312.png)

#### 多模型集成学习

​	经过实验之后发现基于ViT模型和CNN系列模型的串联效果并不明显，那么既然两种模型的分类原理不同，这里借助集成学习的思想并行几个网络进行训练，类似于inception架构同时学习然后通过投票表决的方法对结果进行分类。这里分别以LeNet5,ResNet18和ViT模型进行并行，同时进行训练并对结果进行表决，超过两个及以上认为是一类别即输出为该类别。

![image-20240112193106228](D:\Typora\Pic\image-20240112193106228.png)

### 网络结构

这里网络结构直接看对应模型的代码即可，每一部分将会直接放上模型部分的Python代码。

#### 原图RGB增强，以lenet-5为基础进行实验：

V1版本增强：

只是将输入图像并入后面的每一层，并没有加强层与层之间关系：

```python
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(6, 9, 5, padding=2) # 28x28
        self.pool1 = nn.MaxPool2d(2, 2) # 14x14
        self.conv2 = nn.Conv2d(12, 21, 5) # 10x10
        self.pool2 = nn.MaxPool2d(2, 2) # 5x5
        self.conv3 = nn.Conv2d(24, 120, 5)
        self.fc1 = nn.Linear(864, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        in_size = x.size(0)
        old = x
        out = torch.cat([x, old], dim=1)
        
        out = self.conv1(out) # 24
        out = F.relu(out)
        out = self.pool1(out)  # 12

        transform = transforms.Resize((out.shape[2], out.shape[2]))
        old = transform(x)
        out = torch.cat([out, old], dim=1)
       
        out = self.conv2(out) # 10
        out = F.relu(out)
        out = self.pool2(out)

        transform = transforms.Resize((out.shape[2], out.shape[2]))
        old = transform(x)
        out = torch.cat([out, old], dim=1)
        residual3 = self.conv3(out)

        out = out.view(in_size, -1)
        out = self.fc1(out)
        out = F.relu(out)
        out = self.fc2(out)
        out = F.log_softmax(out, dim=1)
        return out

```

V2版本增强：

不仅将输入图像并入后面每一层，还通过跨层卷积加强了层与层之间关系：

```python
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(6, 9, 5, padding=2) # 28x28
        self.pool1 = nn.MaxPool2d(2, 2) # 14x14
        self.conv2 = nn.Conv2d(12, 21, 5) # 10x10
        self.pool2 = nn.MaxPool2d(2, 2) # 5x5
        self.conv3 = nn.Conv2d(24, 120, 5)
        self.conv11 = nn.Conv2d(6, 24, 5)
        self.conv12 = nn.Conv2d(6, 120, 5)
        self.conv21 = nn.Conv2d(12, 120, 5)
        self.fc1 = nn.Linear(9504, 1024)
        self.fc2 = nn.Linear(1024, 10)

    def forward(self, x):
        in_size = x.size(0)
        old = x
        out = torch.cat([x, old], dim=1)
        out12 = self.conv12(out)
        out = self.conv1(out) # 24
        out = F.relu(out)
        out = self.pool1(out)  # 12

        transform = transforms.Resize((out.shape[2], out.shape[2]))
        old = transform(x)
        out = torch.cat([out, old], dim=1)
        out21 = self.conv21(out)
        
        out = self.conv2(out) # 10
        out = F.relu(out)
        out = self.pool2(out)

        transform = transforms.Resize((out.shape[2], out.shape[2]))
        old = transform(x)
        out12 = transform(out12)
        out21 = transform(out21)
        out = torch.cat([out, old], dim=1)
        residual3 = self.conv3(out)

        out = torch.cat([out, out12, out21], dim=1)
        out = out.view(in_size, -1)
        out = self.fc1(out)
        out = F.relu(out)
        out = self.fc2(out)
        out = F.log_softmax(out, dim=1)
        return out

```

#### 卷积特征提取

原有ViT结构：

```
# 定义Vision Transformer模型
class VisionTransformer(nn.Module):
    def __init__(self, num_classes=10, image_size=32, patch_size=4, hidden_dim=128, num_heads=8, num_layers=2):
        super(VisionTransformer, self).__init__()
        
        self.embedding = nn.Conv2d(3, hidden_dim, kernel_size=patch_size, stride=patch_size)
        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads), num_layers)
        self.fc = nn.Linear(hidden_dim * (image_size // patch_size) ** 2, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        x = x.flatten(2).permute(2, 0, 1)
        x = self.transformer(x)
        x = x.permute(1, 0, 2).flatten(1)
        x = self.fc(x)
        return x
```

Lenet-5作为patch层的VIT结构：

```
# 定义Vision Transformer模型
class VisionTransformer(nn.Module):
    def __init__(self, num_classes=10, image_size=32, patch_size=4, hidden_dim=128, num_heads=8, num_layers=2):
        super(VisionTransformer, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5, padding=2) # 28x28
        self.pool1 = nn.MaxPool2d(2, 2) # 14x14
        self.conv2 = nn.Conv2d(6, 16, 5, padding=2) # 10x10
        self.pool2 = nn.MaxPool2d(2, 2) # 5x5
        self.conv3 = nn.Conv2d(16, 3, 5)

        self.embedding = nn.Conv2d(3, hidden_dim, kernel_size=patch_size, stride=patch_size)
        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads), num_layers)
        self.fc = nn.Linear(hidden_dim * (image_size // patch_size) ** 2, 1024)
        self.fc1 = nn.Linear(1024,128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        out = self.conv1(x) #24
        out = F.relu(out)
        out = self.pool1(out)  #12
        out = self.conv2(out) #10
        out = F.relu(out)
        out = self.pool2(out)
        out = self.conv3(out)
        
        out = F.interpolate(out, size=(32, 32), mode='bilinear', align_corners=False)
        out = self.embedding(x)
        out = out.flatten(2).permute(2, 0, 1)
        out = self.transformer(out)
        out = out.permute(1, 0, 2).flatten(1)
        
        out = self.fc(out)
        out = self.fc1(out)
        out = self.fc2(out)
        return out
```

ResNet18作为patch层的VIT结构：

vit层不变，

```
# 定义ResNet18模型
class ResNet18(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNet18, self).__init__()
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        
        self.layer1 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64)
        )
        
        self.layer2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(128)
        )
        
        self.layer3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(256)
        )
        
        self.layer4 = nn.Sequential(
            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(512)
        )
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
        
        self.identity_conv1 = nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0, bias=False)
        self.identity_conv2 = nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0, bias=False)
        self.identity_conv3 = nn.Conv2d(256 ,512, kernel_size=1, stride=2, padding=0, bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        
        x1 = self.layer1(x)
        x = x + x1
        
        x2 = self.layer2(x1)
        x1_identity = self.identity_conv1(x)
        x = x1_identity + x2
        
        x3 = self.layer3(x)
        x2_identity = self.identity_conv2(x)
        x = x2_identity + x3
        
        x4 = self.layer4(x)
        x3_identity = self.identity_conv3(x)
        x = x3_identity + x4
        
        x = self.avgpool(x)
        return x

class ViT_ResNet18(nn.Module):
    def __init__(self, num_classes=10):
        super(ViT_ResNet18, self).__init__()
        
        # 创建ViT模型
        self.vit = VisionTransformer(num_classes=num_classes) 
        
        # 创建ResNet模型
        self.ResNet = ResNet18()

    def forward(self, x):
        # 使用ResNet模型处理ViT的输出
        x = self.ResNet(x)
        x = F.interpolate(x, size=(32, 32), mode='bilinear', align_corners=False)
        # 使用ViT模型处理输入
        x = self.vit(x)
        return x
```

#### 多模型集成学习

通过并行的策略将几种方法均运用起来，通过投票的方式进行分类，具体每一层网络架构，框架网络架构同研究思路图。

### 优化原则

本课题的优化策略不是重点内容，均采用交叉熵损失加adam进行优化，其他内容同基本神经网络模型。

## 实验与结果分析

### 实验数据集

​	本次实验采用的数据集是CIFAR10数据集，其含有60000张32×32的RGB三通道图片，属于图片数量较小的数据集，在代码中借助torchvision即可进行调用。

### 实验平台

​	实验平台为基于Linux 内核版本3.10.0-1160.88.1.el7.x86_64的操作系统，是一个x86_64架构的Linux内核。相关pip包见requirements.txt。

### 实验内容与结果分析

​	本次实验内容包含三个部分：训练集损失，测试集损失和模型准确度，记录了模型准确度随着epoch轮数变化的文本和三个指标随着epoch轮数变化的图像。

#### 原图RGB增强

|                 | loss变化 | 准确度变化 |
| :-------------: | :------: | :--------: |
|      LeNet      |          |            |
|    大核LeNet    |          |            |
|   V1增强LeNet   |          |            |
|   V2增强LeNet   |          |            |
| V1增强大核LeNet |          |            |
| V2增强大核LeNet |          |            |

#### 卷积特征提取

一层transformer

|                    | loss变化 | 准确度变化 |
| :----------------: | :------: | :--------: |
|       无卷积       |          |            |
|  LeNet5-5辅助卷积  |          |            |
| ResNet-18辅助卷积  |          |            |
| LeNet5大核辅助卷积 |          |            |

两层transformer

|                    | loss变化 | 准确度变化 |
| :----------------: | :------: | :--------: |
|       无卷积       |          |            |
|  LeNet5-5辅助卷积  |          |            |
| ResNet-18辅助卷积  |          |            |
| LeNet5大核辅助卷积 |          |            |

三层transformer及以上

相关研究均停止在两层transformer，三层以上优化难度大，效果增强的不明显，这里没有对transformer进行进一步扩展。

#### 多模型集成学习

模型情况

|                        | loss变化 | 准确度变化 |
| :--------------------: | :------: | :--------: |
|   大核LeNet5 v2增强    |          |            |
|       ResNet-18        |          |            |
| vit无增强双层transform |          |            |
|      集成学习方法      |          |            |

## 结论 

***总结该课题解决问题及意义***

实验总结表：其中单层网络训练epoch为25轮，多层网络训练epoch为50轮，涉及到的准确度均为最高准确度。

|         模型         | 准确度 |                模型                 | 准确度 |
| :------------------: | :----: | :---------------------------------: | :----: |
|      原始LeNet5      |        |   lenet5卷积提取，单层transformer   |        |
|     大核lenet-5      |        |  resnet18卷积提取，单层transformer  |        |
|     原始ResNet18     |        |   lenet5卷积提取，双层transformer   |        |
|     原始单层ViT      |        |  resnet18卷积提取，双层transformer  |        |
|     原始双层ViT      |        | lenet5大核卷积提取，单层transformer |        |
| 原始图像加强V1LeNet5 |        | lenet5大核卷积提取，双层transformer |        |
| 原始图像加强V2LeNet5 |        |            集成学习方法             |        |

### 课题特点



### 方法



### 结果



### 不足及原因



### 对未来工作进行探讨



## 参考文献

[1]Wei Z, Pan H, Li L, et al. DMFormer: Closing the gap Between CNN and Vision Transformers[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.

[2]Lee S H, Lee S, Song B C. Vision transformer for small-size datasets[J]. arxiv preprint arxiv:2112.13492, 2021.

[3]Touvron H, Cord M, Douze M, et al. Training data-efficient image transformers & distillation through attention[C]//International conference on machine learning. PMLR, 2021: 10347-10357.

[4]Mehta S, Rastegari M. MobileViT: light-weight, general-purpose, and mobile-friendly vision transformer[J]. arxiv preprint arxiv:2110.02178, 2021.